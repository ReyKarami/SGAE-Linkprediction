{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e4e94",
   "metadata": {
    "id": "118e4e94"
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from time import gmtime, strftime\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import math\n",
    "import csv\n",
    "from csv import DictWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "import scipy.sparse as ssp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import urllib\n",
    "import io\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score,\n",
    "                             PrecisionRecallDisplay, precision_score, recall_score, accuracy_score, f1_score)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import (ModuleList, Linear, Conv1d, MaxPool1d, Embedding, ReLU, BCEWithLogitsLoss,\n",
    "                      Sequential, BatchNorm1d as BN)\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GAE\n",
    "from torch_geometric.utils import (negative_sampling, add_self_loops,\n",
    "                                   train_test_split_edges)\n",
    "from torch_geometric.datasets import KarateClub, RelLinkPredDataset, LastFM\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "from python_utils.logger import Logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    path = \"data/%s\" %( args.dataset_name)\n",
    "    \n",
    "    isExist = os.path.exists(path + \"/graph_info.csv\")\n",
    "    \n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "\n",
    "            \n",
    "        A, A_train, A_test, data_x, n_v, n_e, train_message_edges, test_message_edges,\\\n",
    "        edge_pos_train, edge_neg_train, edge_pos_test, edge_neg_test =\\\n",
    "            data_loader(args.dataset_name, args.network_type, args.feature_type )\n",
    "\n",
    "\n",
    "        data_train = Data(x = data_x, edge_index = train_message_edges, num_nodes = n_v)\n",
    "        data_test = Data(x = data_x, edge_index = test_message_edges, num_nodes = n_v)\n",
    "        \n",
    "        \n",
    "        \n",
    "        num_subgraph_nodes = math.ceil((2*n_e/n_v)*(1+((2*n_e)/((n_v)*(n_v-1)))))#PLACN\n",
    "        num_nodes = num_subgraph_nodes + math.ceil(0.5*num_subgraph_nodes) + 2\n",
    "        \n",
    "        arrayOfA = A.toarray()\n",
    "        listOfA = arrayOfA.tolist()\n",
    "        \n",
    "        arrayOfA_train = A_train.toarray()\n",
    "        listOfA_train = arrayOfA_train.tolist()\n",
    "        \n",
    "        arrayOfA_test = A_test.toarray()\n",
    "        listOfA_test = arrayOfA_test.tolist()\n",
    "        \n",
    "        field_names = ['adj_matrix', 'adj_matrix_train', 'adj_matrix_test', 'data_x', 'num_vertices', 'num_edges',\n",
    "                        'edge_index', 'train_message_edges', 'test_message_edges', 'edge_pos_train', 'edge_neg_train',\n",
    "                        'edge_pos_test', 'edge_neg_test','num_subgraph_nodes']\n",
    "        \n",
    "        dict = {'adj_matrix':listOfA, 'adj_matrix_train':listOfA_train, 'adj_matrix_test':listOfA_test,\n",
    "                'data_x':data_x.tolist(), 'num_vertices':n_v,  'num_edges':n_e,\n",
    "                'train_message_edges':train_message_edges.tolist(), 'test_message_edges':test_message_edges.tolist(),\n",
    "                'edge_pos_train':edge_pos_train.tolist(),\n",
    "                'edge_neg_train':edge_neg_train.tolist(), 'edge_pos_test':edge_pos_test.tolist(),\n",
    "                'edge_neg_test':edge_neg_test.tolist(), 'num_subgraph_nodes':num_subgraph_nodes}\n",
    "        \n",
    "        with open(path + \"/graph_info.csv\", mode='w') as f_object:\n",
    "            dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "            dictwriter_object.writeheader()\n",
    "            dictwriter_object.writerow(dict)\n",
    "            f_object.close()\n",
    "        train_edges_x = np.concatenate([edge_pos_train.T[0],edge_neg_train.T[0]])\n",
    "        train_edges_y = np.concatenate([edge_pos_train.T[1],edge_neg_train.T[1]])\n",
    "        subgraph_primitive(A_train,train_edges_x , train_edges_y, num_nodes=num_nodes, type_data='train' ) \n",
    "        \n",
    "        test_edges_x = np.concatenate([edge_pos_test.T[0],edge_neg_test.T[0]])\n",
    "        test_edges_y = np.concatenate([edge_pos_test.T[1],edge_neg_test.T[1]])\n",
    "        subgraph_primitive(A_test, test_edges_x, test_edges_y, num_nodes=num_nodes, type_data='test' ) \n",
    "\n",
    "    else:\n",
    "        \n",
    "        df = pd.read_csv(path+\"/graph_info.csv\")\n",
    "\n",
    "        A = ssp.csr_matrix(ast.literal_eval(df['adj_matrix'].dropna().values[0]))\n",
    "        A_train = ssp.csr_matrix(ast.literal_eval(df['adj_matrix_train'].dropna().values[0]))\n",
    "        A_test = ssp.csr_matrix(ast.literal_eval(df['adj_matrix_test'].dropna().values[0]))\n",
    "        data_x = torch.tensor(ast.literal_eval(df['data_x'].dropna().values[0]))\n",
    "        train_message_edges = torch.tensor(ast.literal_eval(df['train_message_edges'].dropna().values[0]))\n",
    "        test_message_edges = torch.tensor(ast.literal_eval(df['test_message_edges'].dropna().values[0]))\n",
    "        edge_pos_train = torch.tensor(ast.literal_eval(df['edge_pos_train'].dropna().values[0]))\n",
    "        edge_neg_train = torch.tensor(ast.literal_eval(df['edge_neg_train'].dropna().values[0]))\n",
    "        edge_pos_test = torch.tensor(ast.literal_eval(df['edge_pos_test'].dropna().values[0]))\n",
    "        edge_neg_test = torch.tensor(ast.literal_eval(df['edge_neg_test'].dropna().values[0]))\n",
    "        n_v = df['num_vertices'].dropna().values[0]\n",
    "        n_e = df['num_edges'].dropna().values[0]\n",
    "        num_subgraph_nodes = df['num_subgraph_nodes'].dropna().values[0]\n",
    "        num_nodes = num_subgraph_nodes + math.ceil(0.5*num_subgraph_nodes) + 2\n",
    "        \n",
    "        data_train = Data(x = data_x, edge_index = train_message_edges, num_nodes = n_v)\n",
    "        data_test = Data(x = data_x, edge_index = test_message_edges, num_nodes = n_v)\n",
    "        \n",
    "            \n",
    "    \n",
    "    print(\"number of nodes:\", n_v)  \n",
    "    print(\"number of edges:\", n_e) \n",
    "    print(\"number of message edges in train:\", len(train_message_edges))\n",
    "    print(\"number of message edges in test:\", len(test_message_edges))\n",
    "    print(\"number of supervision edges in train:\", len(edge_pos_train))\n",
    "    print(\"number of supervision edges in test:\", len(edge_pos_test))\n",
    "    \n",
    "    print(\"number of subgraph nodes :\", num_subgraph_nodes)\n",
    "    \n",
    "    return(data_train, data_test, A, A_train, A_test, edge_pos_train,\n",
    "           edge_neg_train, edge_pos_test, edge_neg_test, num_subgraph_nodes, num_nodes)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97656ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec_model(graph):\n",
    "    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous() \n",
    "    node2vec = Node2Vec(edge_index, embedding_dim=256, walk_length=40, context_size=10, walks_per_node=100)\n",
    "    loader = DataLoader(range(graph.number_of_nodes()), batch_size=64, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(node2vec.parameters(), lr=0.01) \n",
    "    node2vec.train()\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            pos_rw = node2vec.pos_sample(batch) \n",
    "            neg_rw = node2vec.neg_sample(batch)  \n",
    "            \n",
    "            loss = node2vec.loss(pos_rw, neg_rw)\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    node2vec.eval()  \n",
    "    embed = node2vec.embedding.weight.data.cpu().numpy() \n",
    "\n",
    "    data_x = torch.tensor(embed)\n",
    "    print(data_x)\n",
    "    return(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_name, network_type, feature_type, negative_injection = True):\n",
    "    \n",
    "    print(\"load data...\")\n",
    "    file_path = \"dataset/\" + args.dataset_name + \".txt\"\n",
    "    \n",
    "    #sample positive\n",
    "    positive_all = np.loadtxt(file_path, dtype=int, usecols=(0, 1))\n",
    "    if np.min(positive_all) == 1:\n",
    "        positive_all -= 1\n",
    "    np.random.shuffle(positive_all)\n",
    "    n = int(len(positive_all)*0.54)\n",
    "    positive = np.asarray(positive_all[:n])\n",
    "    supervision_edges_pos = positive\n",
    "    message_edges = np.asarray(positive_all[n:])\n",
    "    \n",
    "    G = nx.Graph() if args.network_type == 0 else nx.DiGraph()\n",
    "    G.add_edges_from(positive_all)\n",
    "    nodes_size = len(G.nodes()) #nodes size in the network\n",
    "    edge_size = len(G.edges())\n",
    "    \n",
    "    # sample negative\n",
    "    negative_all = list(nx.non_edges(G))\n",
    "    if np.min(negative_all) == 1:\n",
    "        negative_all -= 1\n",
    "    np.random.shuffle(negative_all)\n",
    "    negative = np.asarray(negative_all[:len(positive)])\n",
    "    supervision_edges_neg = negative\n",
    "    \n",
    "    test_size = int(len(positive) * args.test_ratio)\n",
    "    train_pos, test_pos = supervision_edges_pos[:-test_size], supervision_edges_pos[-test_size:]\n",
    "    train_neg, test_neg = supervision_edges_neg[:-test_size], supervision_edges_neg[-test_size:]\n",
    "    \n",
    "    train_message_edges = message_edges\n",
    "    test_message_edges = np.concatenate([message_edges, train_pos])\n",
    "    \n",
    "    #adj matrix\n",
    "    A = np.zeros([nodes_size, nodes_size], dtype=np.uint8)\n",
    "    A[positive_all[:, 0], positive_all[:, 1]] = 1\n",
    "    \n",
    "    if network_type == 0:\n",
    "        A[positive_all[:, 1], positive_all[:, 0]] = 1\n",
    "        \n",
    "    \n",
    "    A_train = np.zeros([nodes_size, nodes_size], dtype=np.uint8)\n",
    "    A_train[train_message_edges[:, 0], train_message_edges[:, 1]] = 1\n",
    "    \n",
    "        \n",
    "    if network_type == 0:\n",
    "        A_train[train_message_edges[:, 1], train_message_edges[:, 0]] = 1\n",
    "    \n",
    "    \n",
    "    A_test = np.zeros([nodes_size, nodes_size], dtype=np.uint8)\n",
    "    A_test[test_message_edges[:, 0], test_message_edges[:, 1]] = 1\n",
    "    if network_type == 0:\n",
    "        A_test[test_message_edges[:, 1], test_message_edges[:, 0]] = 1\n",
    "        \n",
    "    for i in range(nodes_size):\n",
    "        A_test[i,i] = 1\n",
    "        A_train[i,i] = 1\n",
    "        A[i,i] = 1\n",
    "        \n",
    "    A = ssp.csr_matrix(A)    \n",
    "    A_test = ssp.csr_matrix(A_test) \n",
    "    A_train = ssp.csr_matrix(A_train)\n",
    "    \n",
    "    #nodes feature\n",
    "    if feature_type == \"node2vec\":\n",
    "        data_x = node2vec_model(G)\n",
    "\n",
    "    elif feature_type == \"onehot\":\n",
    "        data_x = torch.diag(torch.ones(nodes_size))#ONE_HOT\n",
    "    \n",
    "            \n",
    "    return(A ,A_train, A_test, data_x, nodes_size, edge_size,\n",
    "               torch.from_numpy(train_message_edges), torch.from_numpy(test_message_edges), \n",
    "               torch.from_numpy(train_pos), torch.from_numpy(train_neg),\n",
    "              torch.from_numpy(test_pos), torch.from_numpy(test_neg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287d188",
   "metadata": {
    "id": "5287d188"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "\n",
    "    accu = 0.0\n",
    "\n",
    "    pred_label = pred.ge(0.5) \n",
    "    accu = torch.sum(pred_label == label) / label.shape[0]\n",
    "    accu = accu.item()\n",
    "\n",
    "    return round(accu,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred, label):\n",
    "\n",
    "    pred_label = pred.ge(0.5) \n",
    "    recall = recall_score(label, pred_label)\n",
    "\n",
    "    return round(recall,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25943e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, label):\n",
    "\n",
    "    pred_label = pred.ge(0.5) \n",
    "    precision = precision_score(label, pred_label)\n",
    "\n",
    "    return round(precision,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a388af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "\n",
    "    pred_label = pred.ge(0.5) \n",
    "    f1 = f1_score(label, pred_label)\n",
    "\n",
    "    return round(f1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0548106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(pred, label):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(label, pred)\n",
    "    \n",
    "    return (precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_AUC(pred, label):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(label, pred)\n",
    "    \n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    return round(pr_auc,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0bc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(pred, label):\n",
    "    \n",
    "    avg_precision = average_precision_score(label, pred)\n",
    "    \n",
    "    return round(avg_precision,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neighbors(fringe, A, outgoing=True):\n",
    "\n",
    "    if outgoing:\n",
    "        res = (set(A[list(fringe)].indices))\n",
    "    else:\n",
    "        res = set(A[:, list(fringe)].indices)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_hop_subgraph(num_hops, A, src, dst=None, num_nodes=0 ,sample_ratio=1.0, \n",
    "                   max_nodes_per_hop=None, node_features=None, \n",
    "                   y=1, directed=False, A_csc=None):\n",
    "    # Extract the k-hop enclosing subgraph around link (src, dst) from A. \n",
    "    if dst == None:\n",
    "        src = src.item()\n",
    "        nodes = [src]\n",
    "        visited = set([src])\n",
    "        fringe = set([src])\n",
    "        for dist in range(1, num_hops+1):\n",
    "            if not directed:\n",
    "                fringe = neighbors(fringe, A)\n",
    "            else:\n",
    "                out_neighbors = neighbors(fringe, A)\n",
    "                in_neighbors = neighbors(fringe, A_csc, False)\n",
    "                fringe = out_neighbors.union(in_neighbors)\n",
    "            fringe = fringe - visited\n",
    "            visited = visited.union(fringe)\n",
    "            if sample_ratio < 1.0:\n",
    "                fringe = random.sample(fringe, int(sample_ratio*len(fringe)))\n",
    "            if max_nodes_per_hop is not None:\n",
    "                if max_nodes_per_hop < len(fringe):\n",
    "                    fringe = random.sample(fringe, max_nodes_per_hop)\n",
    "            if len(fringe) == 0:\n",
    "                break\n",
    "            if num_nodes != 0:\n",
    "                if len(nodes + list(fringe)) > num_nodes:\n",
    "                    x = len(nodes + list(fringe)) - num_nodes\n",
    "                    nodes = nodes + list(fringe)[:-x]\n",
    "                    break\n",
    "                    \n",
    "                if len(nodes) == num_nodes:\n",
    "                    break\n",
    "                    \n",
    "            nodes = nodes + list(fringe)\n",
    "            \n",
    "            \n",
    "        subgraph = A[nodes, :][:, nodes]\n",
    "\n",
    "        # Remove target link between the subgraph.\n",
    "        subgraph[0, 1] = 0\n",
    "        subgraph[1, 0] = 0\n",
    "\n",
    "        if node_features is not None:\n",
    "            node_features = node_features[nodes]\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        src = src.item()\n",
    "        dst = dst.item()\n",
    "        nodes = [src, dst]\n",
    "        visited = set([src, dst])\n",
    "        fringe = set([src, dst])\n",
    "        for dist in range(1, num_hops+1):\n",
    "            if not directed:\n",
    "                fringe = neighbors(fringe, A)\n",
    "            else:\n",
    "                out_neighbors = neighbors(fringe, A)\n",
    "                in_neighbors = neighbors(fringe, A_csc, False)\n",
    "                fringe = out_neighbors.union(in_neighbors)\n",
    "            fringe = fringe - visited\n",
    "            visited = visited.union(fringe)\n",
    "            if sample_ratio < 1.0:\n",
    "                fringe = random.sample(fringe, int(sample_ratio*len(fringe)))\n",
    "            if max_nodes_per_hop is not None:\n",
    "                if max_nodes_per_hop < len(fringe):\n",
    "                    fringe = random.sample(fringe, max_nodes_per_hop)\n",
    "            if len(fringe) == 0:\n",
    "                break\n",
    "                \n",
    "            if num_nodes != 0:\n",
    "                if len(nodes + list(fringe)) > num_nodes:\n",
    "                    x = len(nodes + list(fringe)) - num_nodes\n",
    "                    nodes = nodes + list(fringe)[:-x]\n",
    "                    break\n",
    "                    \n",
    "                if len(nodes) == num_nodes:\n",
    "                    break\n",
    "                    \n",
    "            nodes = nodes + list(fringe)\n",
    "            \n",
    "        subgraph = A[nodes, :][:, nodes]\n",
    "\n",
    "        # Remove target link between the subgraph.\n",
    "        subgraph[0, 1] = 0\n",
    "        subgraph[1, 0] = 0\n",
    "\n",
    "        if node_features is not None:\n",
    "            node_features = node_features[nodes]\n",
    "\n",
    "    return nodes, subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab982d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_primitive(A, nodes1, nodes2, num_nodes, type_data ):\n",
    "    \n",
    "    max_hop= args.max_hop\n",
    "    \n",
    "    \n",
    "        \n",
    "    path = \"data/%s\" %( args.dataset_name)\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "    field_names = ['nodes1', 'nodes2', 'subgraph_nodes', 'subgraph_A']\n",
    "\n",
    "    for index in range(len(nodes1)): \n",
    "        subgraph_nodes, subgraph_A = k_hop_subgraph(max_hop, A, nodes1[index], nodes2[index], num_nodes=num_nodes)\n",
    "\n",
    "        dict = {'nodes1':subgraph_nodes[0], 'nodes2':subgraph_nodes[1], 'subgraph_nodes':subgraph_nodes, 'subgraph_A':subgraph_A}\n",
    "\n",
    "        isExist = os.path.exists(path+'/subgraphs_%s_info.csv'%(type_data))\n",
    "        if not isExist:\n",
    "            with open(path+'/subgraphs_%s_info.csv'%(type_data), mode='a') as f_object:\n",
    "                dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwriter_object.writeheader()\n",
    "                dictwriter_object.writerow(dict)\n",
    "                f_object.close()\n",
    "        else:\n",
    "            with open(path+'/subgraphs_%s_info.csv'%(type_data), mode='a') as f_object:\n",
    "                dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwriter_object.writerow(dict)\n",
    "                f_object.close()\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph2vec(A, dff, embed, nodes1, nodes2, num_nodes  ):\n",
    "    \n",
    "    z_embed = torch.tensor(()).to(embed.device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.dist_type == 'cos':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "        \n",
    "    for index in range(len(nodes1)):\n",
    "        dis = []\n",
    "        \n",
    "        node1 = nodes1[index].item()\n",
    "        node2 = nodes2[index].item()\n",
    "        \n",
    "           \n",
    "        subgraph_nodes = (dff[(dff['nodes1'] == node1)&(dff['nodes2'] == node2)]['subgraph_nodes'])\n",
    "        if args.subgraph_type == 'DIS':\n",
    "            subgraph_nodes = (ast.literal_eval(subgraph_nodes.dropna().values[0]))\n",
    "        elif args.subgraph_type == 'hhop':\n",
    "            subgraph_nodes = (ast.literal_eval(subgraph_nodes.dropna().values[0]))[:num_nodes+2]\n",
    "        \n",
    "        embed_node_index = ((embed[node1]+embed[node2])/2)\n",
    "        embed_node1 = (embed[node1])\n",
    "        embed_node2 = (embed[node2])\n",
    "\n",
    "\n",
    "        d_list = []\n",
    "        for i in range (len(subgraph_nodes)):\n",
    "\n",
    "            if subgraph_nodes[i] != nodes1[index] and subgraph_nodes[i] != nodes2[index]:\n",
    "\n",
    "                if args.dist_type == 'norm':\n",
    "                    d_list.append((torch.norm((embed[subgraph_nodes[i]])-(embed_node_index))).item())\n",
    "\n",
    "                elif args.dist_type == 'cos':\n",
    "                    d_list.append(1-(cos(embed[subgraph_nodes[i]], embed_node_index).item()))\n",
    "            else:\n",
    "                d_list.append(0)\n",
    "\n",
    "        d_list = torch.tensor((normalize([d_list])[0]).tolist())\n",
    "\n",
    "\n",
    "        for i in range (len(subgraph_nodes)):\n",
    "            dis.append([subgraph_nodes[i], d_list[i]])\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.DataFrame.from_dict(dict(dis), orient='index', columns=['distance'])\n",
    "        df = df.sort_values('distance', ascending=True)\n",
    "\n",
    "\n",
    "        subgraph = df.dropna().index.values[:(num_nodes+2)]\n",
    "        distances = df.dropna().values[:(num_nodes+2)]\n",
    "\n",
    "        subgraph_embed = embed[subgraph].to(embed.device)\n",
    "\n",
    "        if args.subgraph_feature_type == 'NDP':\n",
    "\n",
    "            w_list = torch.zeros(len(subgraph))\n",
    "\n",
    "            for i in range(len(subgraph)):\n",
    "                d = distances[i,0]\n",
    "                if d==0:\n",
    "                    w_list[i] = math.log((1-0.0001)/0.0001)\n",
    "                elif(d==1):\n",
    "                    w_list[i] = math.log((1-0.9999)/0.9999)\n",
    "                else:\n",
    "                    w_list[i] = math.log((1-d)/d) #adaboost\n",
    "            w_list = w_list / (w_list.sum())\n",
    "\n",
    "            for i in range(len(subgraph)):\n",
    "                subgraph_embed[i] = subgraph_embed[i]*w_list[i]\n",
    "\n",
    "            s = (subgraph_embed.sum(dim=0))\n",
    "            s = s.reshape([1,len(s)])\n",
    "            z_embed = torch.cat((z_embed,s), 0)\n",
    "\n",
    "\n",
    "\n",
    "        elif args.subgraph_feature_type == 'cnn':\n",
    "            subgraph_embed= subgraph_embed.unsqueeze(0)\n",
    "\n",
    "            if len(subgraph_embed)<(num_nodes+2):\n",
    "                s = subgraph_embed.tolist()\n",
    "\n",
    "                for x in range(0,((num_nodes+2)-len(subgraph))):\n",
    "                    z = [0]*args.GNN_out_channels\n",
    "                    s[0].append(z)\n",
    "\n",
    "                subgraph_embed = torch.tensor(s).to(embed.device)\n",
    "\n",
    "            z_embed = torch.cat((z_embed,subgraph_embed),0)\n",
    "            \n",
    "    \n",
    "    return(z_embed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subg2vec_model(torch.nn.Module):\n",
    "    def __init__(self, input_num, feature_dim, hidden_channels, out_channels, dropout):\n",
    "        \n",
    "        super(subg2vec_model, self).__init__()        \n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, (3,feature_dim), padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, (3,2), padding=1)\n",
    "        \n",
    "        input_num = input_num+2\n",
    "        \n",
    "        x = math.ceil((math.ceil((input_num-3+2)/2)-3+2)/2)\n",
    "        y = math.ceil((math.ceil((feature_dim-2+2)/2)-2+2)/2)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(int(0.5*x*y), hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return (x.squeeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b8a33",
   "metadata": {
    "id": "997b8a33"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        \n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "    def forward(self, x, adj):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, GNN_in_channels, GNN_hidden_channels, GNN_out_channels,\n",
    "                 GNN_num_layers, dropout):\n",
    "\n",
    "        super(GAutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "        self.encoders.append(GCNConv(GNN_in_channels, GNN_hidden_channels))\n",
    "        for _ in range(GNN_num_layers - 2):\n",
    "            self.encoders.append(GCNConv(GNN_hidden_channels, GNN_hidden_channels))\n",
    "        self.encoders.append(GCNConv(GNN_hidden_channels, GNN_out_channels))\n",
    "        \n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for encoder in self.encoders:\n",
    "            encoder.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for encoder in self.encoders[:-1]:\n",
    "            x = encoder(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.encoders[-1](x, adj) \n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58aea3",
   "metadata": {
    "id": "ad58aea3"
   },
   "outputs": [],
   "source": [
    "def predictor_simple(z, edge_index):\n",
    "    #print('start decoder')\n",
    "    z1 = (z[edge_index[0].long()])\n",
    "    z2 = (z[edge_index[1].long()])\n",
    "    logits = (z1 * z2).sum(dim=-1)\n",
    "    #print(logits)\n",
    "    #print('end decoder')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db420367",
   "metadata": {
    "id": "db420367"
   },
   "outputs": [],
   "source": [
    "class predictor_model(torch.nn.Module):\n",
    "    def __init__(self, linear_in_channels, linear_hidden_channels, linear_num_layers, dropout):\n",
    "        \n",
    "        super(predictor_model, self).__init__()        \n",
    "        \n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(3*linear_in_channels, linear_hidden_channels))\n",
    "        for _ in range(linear_num_layers-2):\n",
    "            self.lins.append(torch.nn.Linear(linear_hidden_channels, linear_hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(linear_hidden_channels, 1))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "            \n",
    "    def forward(self, z):\n",
    "\n",
    "        for lin in self.lins[:-1]:\n",
    "            z = lin(z)\n",
    "            z = F.relu(z)\n",
    "            z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "        z = self.lins[-1](z)\n",
    "        return (((z)).T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdf3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictor_model_cnn(torch.nn.Module):\n",
    "    def __init__(self, input_num, feature_dim, hidden_channels, out_channels, dropout):\n",
    "        \n",
    "        super(predictor_model_cnn, self).__init__()        \n",
    "        \n",
    "               \n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, (3,5), padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, (3,5), padding=1)\n",
    "        \n",
    "        input_num = input_num+2\n",
    "        x = math.ceil((math.ceil((input_num-3+2)/2)-3+2)/2)\n",
    "        y = math.ceil((math.ceil((feature_dim-5+2)/2)-5+2)/2)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(16*x*y, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 16)\n",
    "        self.lin4 = torch.nn.Linear(16, 1)\n",
    "\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "        self.lin3.reset_parameters()\n",
    "        self.lin4.reset_parameters()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = self.lin4(x)\n",
    "        return (x.squeeze(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f4f35",
   "metadata": {
    "id": "e96f4f35"
   },
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324ec67",
   "metadata": {
    "id": "2324ec67"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train( model, model_predictor, data, A, data_split, optimizer,\n",
    "          optimizer_predictor, num_subgraph_nodes, model_subgraph=None, optimizer_subgraph=None):\n",
    "    \n",
    "    model.train()\n",
    "    model_predictor.train()\n",
    "    if args.subgraph_feature_type == 'cnn':\n",
    "        model_subgraph.train()\n",
    "    \n",
    "    pos_train_edge = data_split['train']['edge'].to(data.x.device)\n",
    "    neg_train_edge = data_split['train']['edge_neg'].to(data.x.device)\n",
    "    link_labels = ((get_link_labels(pos_train_edge.T, neg_train_edge.T)).to(data.x.device)).tolist()\n",
    "    \n",
    "    train_edge = (torch.cat((pos_train_edge, neg_train_edge), dim=0)).tolist()\n",
    "    \n",
    "    train_data = tuple(zip(train_edge,link_labels))\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    optimizer_predictor.zero_grad()\n",
    "    if args.subgraph_feature_type == 'cnn':\n",
    "        optimizer_subgraph.zero_grad()\n",
    "        \n",
    "\n",
    "    \n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "    pbar = tqdm(train_loader, ncols=70)\n",
    "    for perm in pbar:\n",
    "        if args.GNN_type == 'autoencoder':\n",
    "            z = model.encode(data.x, data.edge_index.T)\n",
    "        else:\n",
    "            z = model(data.x, data.edge_index.T)\n",
    "        i += 1\n",
    "\n",
    "        edge_index_x = perm[0][0].to(data.x.device)\n",
    "        edge_index_y = perm[0][1].to(data.x.device)\n",
    "        link_labels = perm[1].to(data.x.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        z1 = (z[edge_index_x.long()])\n",
    "        z2 = (z[edge_index_y.long()])\n",
    "        z_nodes = torch.cat([z1,z2], dim=-1)\n",
    "        \n",
    "        df = pd.read_csv(\"data/%s\" %(args.dataset_name) + '/subgraphs_%s_info.csv'%('train'))\n",
    "        \n",
    "        if args.subgraph_feature_type == 'cnn':\n",
    "            sub_embed = subgraph2vec(A, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data.x.device)\n",
    "            z_sub = model_subgraph(sub_embed)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "            \n",
    "        elif args.subgraph_feature_type == 'cnn2':\n",
    "            z_embed = subgraph2vec(A, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data.x.device)\n",
    "\n",
    "        else:\n",
    "            z_sub = subgraph2vec(A, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data.x.device)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "\n",
    "        link_logits = model_predictor(z_embed).to(data.x.device)\n",
    "        \n",
    "        \n",
    "        link_logits = link_logits.squeeze()    \n",
    "\n",
    "        if args.GNN_type == 'autoencoder':\n",
    "            loss_autoencoder = model.recon_loss(z, data.edge_index.T)\n",
    "            loss_predict = BCEWithLogitsLoss()(link_logits, link_labels)\n",
    "            loss = loss_autoencoder+loss_predict\n",
    "            loss.backward()\n",
    "\n",
    "        else:\n",
    "            loss = BCEWithLogitsLoss()(link_logits, link_labels)\n",
    "            loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model_predictor.parameters(), 2.0)\n",
    "        if args.subgraph_feature_type == 'cnn':\n",
    "            torch.nn.utils.clip_grad_norm_(model_subgraph.parameters(), 2.0)\n",
    "\n",
    "        num_examples = (link_logits.size(0))\n",
    "        total_loss += loss.item() * num_examples\n",
    "        total_examples += num_examples\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer_predictor.step()\n",
    "        if args.subgraph_feature_type == 'cnn':\n",
    "            optimizer_subgraph.step()\n",
    "\n",
    "    return (total_loss/ total_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135bfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, model_predictor, data_train, data_test, A_train, A_test, split_edge, num_subgraph_nodes,\n",
    "         model_subgraph=None, evaluator=False):\n",
    "    \n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    model_predictor.eval()\n",
    "    if args.subgraph_feature_type == 'cnn':\n",
    "        model_subgraph.eval()\n",
    "\n",
    "\n",
    "\n",
    "    pos_train_edge = split_edge['train']['edge'].T\n",
    "    neg_train_edge = split_edge['train']['edge_neg'].T\n",
    "    pos_test_edge = split_edge['test']['edge'].T\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].T\n",
    "    \n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    \n",
    "\n",
    "    train_preds = []\n",
    "    train_labels = (get_link_labels(pos_train_edge, neg_train_edge)).to(data_train.x.device)\n",
    "    train_edge = torch.cat((pos_train_edge, neg_train_edge), dim=1)\n",
    "    \n",
    "    if args.GNN_type == 'autoencoder':\n",
    "        z = model.encode(data_train.x, data_train.edge_index.T)\n",
    "    else:\n",
    "        z = model(data_train.x, data_train.edge_index.T)\n",
    "            \n",
    "    for perm in DataLoader(range(train_edge.size(1)), batch_size, shuffle=False):\n",
    "        \n",
    "        \n",
    "        edge = (train_edge.T[perm]).T\n",
    "        \n",
    "        edge_index_x = edge[0]\n",
    "        edge_index_y = edge[1]\n",
    "        \n",
    "        z1 = (z[edge_index_x.long()])\n",
    "        z2 = (z[edge_index_y.long()])\n",
    "        z_nodes = torch.cat([z1,z2], dim=-1)\n",
    "        \n",
    "        df = pd.read_csv(\"data/%s\" %( args.dataset_name) + '/subgraphs_%s_info.csv'%('train'))\n",
    "        \n",
    "        if args.subgraph_feature_type == 'cnn':\n",
    "            sub_embed = subgraph2vec(A_train, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_sub = model_subgraph(sub_embed)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "            \n",
    "        elif args.subgraph_feature_type == 'cnn2':\n",
    "            z_embed = subgraph2vec(A_train, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data_train.x.device)\n",
    "\n",
    "        else:\n",
    "            z_sub = subgraph2vec(A_train, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)        \n",
    "        \n",
    "            \n",
    "        \n",
    "        train_preds += [model_predictor(z_embed)]\n",
    "        \n",
    "    train_pred = torch.cat(train_preds, dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "    test_preds = []\n",
    "    test_labels = (get_link_labels(pos_test_edge, neg_test_edge)).to(data_test.x.device)\n",
    "    test_edge = torch.cat((pos_test_edge, neg_test_edge), dim=1)\n",
    "    \n",
    "    if args.GNN_type == 'autoencoder':\n",
    "        z = model.encode(data_test.x, data_test.edge_index.T)\n",
    "    else:\n",
    "        z = model(data_test.x, data_test.edge_index.T)\n",
    "        \n",
    "    for perm in DataLoader(range(test_edge.size(1)), batch_size, shuffle=False):\n",
    "            \n",
    "        edge = (test_edge.T[perm]).T\n",
    "        \n",
    "        edge_index_x = edge[0]\n",
    "        edge_index_y = edge[1]\n",
    "        \n",
    "        z1 = (z[edge_index_x.long()])\n",
    "        z2 = (z[edge_index_y.long()])\n",
    "        z_nodes = torch.cat([z1,z2], dim=-1)\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv(\"data/%s\" %(args.dataset_name) + '/subgraphs_%s_info.csv'%('test'))\n",
    "        \n",
    "        if args.subgraph_feature_type == 'cnn':\n",
    "            sub_embed = subgraph2vec(A_test,df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_sub = model_subgraph(sub_embed)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "            \n",
    "        elif args.subgraph_feature_type == 'cnn2':\n",
    "            z_embed = subgraph2vec(A_test, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data_train.x.device)\n",
    "\n",
    "        else:\n",
    "            z_sub = subgraph2vec(A_test, df, z, edge_index_x, edge_index_y, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1) \n",
    "            \n",
    "        \n",
    "        test_preds += [model_predictor(z_embed)]\n",
    "        \n",
    "    test_pred = torch.cat(test_preds, dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    train_logits = train_pred\n",
    "    test_logits = test_pred\n",
    "    \n",
    "    accu_train = accuracy(train_logits, train_labels)\n",
    "    accu_test = accuracy(test_logits, test_labels)\n",
    "\n",
    "    recall_train = recall(train_logits[0], train_labels)\n",
    "    recall_test = recall(test_logits[0], test_labels)\n",
    "    \n",
    "    precision_train = precision(train_logits[0], train_labels)\n",
    "    precision_test = precision(test_logits[0], test_labels)\n",
    "    \n",
    "    f1_train = f1(train_logits[0], train_labels)\n",
    "    f1_test = f1(test_logits[0], test_labels)\n",
    "    \n",
    "    avg_precision_train = average_precision(train_logits[0], train_labels)\n",
    "    avg_precision_test = average_precision(test_logits[0], test_labels)\n",
    "    \n",
    "    pr_auc_train = precision_recall_AUC(train_logits[0], train_labels)\n",
    "    pr_auc_test = precision_recall_AUC(test_logits[0], test_labels)\n",
    "    \n",
    "    precision_list_train, recall_list_train = precision_recall(train_logits[0], train_labels)\n",
    "    precision_list_train = precision_list_train.tolist()\n",
    "    recall_list_train = recall_list_train.tolist()\n",
    "    precision_list_test, recall_list_test = precision_recall(test_logits[0], test_labels)\n",
    "    precision_list_test = precision_list_test.tolist()\n",
    "    recall_list_test = recall_list_test.tolist()\n",
    "    \n",
    "    results = {'train': {'accu_train': accu_train, 'recall_train':recall_train, 'precision_train':precision_train,\n",
    "                        'f1_train':f1_train, 'avg_precision_train':avg_precision_train, 'pr_auc_train':pr_auc_train,\n",
    "                        'precision_list_train':precision_list_train, 'recall_list_train':recall_list_train}, \n",
    "                'test': {'accu_test': accu_test, 'recall_test':recall_test, 'precision_test':precision_test,\n",
    "                        'f1_test':f1_test, 'avg_precision_test':avg_precision_test, 'pr_auc_test':pr_auc_test,\n",
    "                        'precision_list_test':precision_list_test, 'recall_list_test':recall_list_test}}\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(device=0, dataset_name='karate', subgraph_type='hhop', dist_type='norm', \n",
    "                    network_type=0, feature_type='onehot', subgraph_feature_type='CNN',\n",
    "                    negative_injection=True, log_steps=1, GNN_type='gcn', GNN_num_layers=3, GNN_hidden_channels=128,\n",
    "                    GNN_out_channels=128, linear_num_layers=5, linear_hidden_channels=32, n2v_dim=256,\n",
    "                    subg2vec_hidden_channels=128, subg2vec_out_channels=128, max_hop=10, dropout=0.0, no_start_run=1,\n",
    "                    batch_size=50, lr=0.0001 ,epochs=300, eval_steps=300, test_ratio=0.1, fold=True, kfolds=5, runs=10,\n",
    "                    coefficient=50, graphlet_size=4, label='dist'):\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='SGAE_subgraph')\n",
    "    parser.add_argument('--device', type=int, default=device)\n",
    "    parser.add_argument('--dataset_name', type=str, default=dataset_name) #'ogbl-collab', 'football', 'FB15k-237',\n",
    "    #'karate', 'USAir', 'PB'\n",
    "    parser.add_argument('--subgraph_type', type=str, default=subgraph_type) # 'hhop', 'DIS'\n",
    "    parser.add_argument('--dist_type', type=str, default=dist_type)#'norm', 'cos'\n",
    "    parser.add_argument('--network_type', type=int, default=network_type)#if directed -> 0, if undirected -> 1\n",
    "    parser.add_argument('--feature_type', type=str, default=feature_type)#'node2vec', 'onehot'\n",
    "    parser.add_argument('--subgraph_feature_type', type=str, default=subgraph_feature_type)#'cnn', 'NDP'\n",
    "    parser.add_argument('--n2v_dim', type=int, default=n2v_dim)\n",
    "    parser.add_argument('--negative_injection', type=bool, default=negative_injection)\n",
    "    parser.add_argument('--log_steps', type=int, default=log_steps)\n",
    "    parser.add_argument('--GNN_type', type=str, default=GNN_type)#'gcn', 'sage', 'autoencoder'\n",
    "    parser.add_argument('--GNN_num_layers', type=int, default=GNN_num_layers)\n",
    "    parser.add_argument('--GNN_hidden_channels', type=int, default=GNN_hidden_channels)\n",
    "    parser.add_argument('--GNN_out_channels', type=int, default=GNN_out_channels)\n",
    "    parser.add_argument('--linear_num_layers', type=int, default=linear_num_layers)\n",
    "    parser.add_argument('--linear_hidden_channels', type=int, default=linear_hidden_channels)\n",
    "    parser.add_argument('--subg2vec_hidden_channels', type=int, default=subg2vec_hidden_channels)\n",
    "    parser.add_argument('--subg2vec_out_channels', type=int, default=subg2vec_out_channels)\n",
    "    parser.add_argument('--max_hop', type=int, default=max_hop)\n",
    "    parser.add_argument('--dropout', type=float, default=dropout)\n",
    "    parser.add_argument('--batch_size', type=int, default=batch_size)\n",
    "    parser.add_argument('--lr', type=float, default=lr)\n",
    "    parser.add_argument('--epochs', type=int, default=epochs)\n",
    "    parser.add_argument('--eval_steps', type=int, default=eval_steps)\n",
    "    parser.add_argument('--test_ratio', type=float, default=test_ratio)\n",
    "    parser.add_argument('--fold', type=bool, default=fold)\n",
    "    parser.add_argument('--kfolds', type=int, default=kfolds)\n",
    "    parser.add_argument('--runs', type=int, default=runs)\n",
    "    parser.add_argument('--coefficient', type=int, default=coefficient)\n",
    "    parser.add_argument('--no_start_run', type=int, default=no_start_run)\n",
    "    parser.add_argument('--graphlet_size',type=int,default=4, help='Maximal graphlet size.')\n",
    "    parser.add_argument('--label', type=str, default=label)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    return(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa087f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    print(args)\n",
    "    \n",
    "#*******dataset*******\n",
    "    \n",
    "    data_train, data_test, A, A_train, A_test, edge_pos_train,\\\n",
    "        edge_neg_train, edge_pos_test, edge_neg_test, num_subgraph_nodes, num_nodes = dataset()\n",
    "    \n",
    "\n",
    "    \n",
    "    split_edge = {'train': {'edge':edge_pos_train, \n",
    "                                'edge_neg':edge_neg_train},\n",
    "                                'test':{'edge':edge_pos_test, \n",
    "                                'edge_neg':edge_neg_test} }\n",
    "            \n",
    "    \n",
    "\n",
    "    device = 'cpu'\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    data_train = data_train.to(device)\n",
    "    data_test = data_test.to(device)\n",
    "\n",
    "#*******create models*******   \n",
    "    if args.GNN_type == 'gcn':\n",
    "        model = GCN(data_train.x.shape[1], args.GNN_hidden_channels, args.GNN_out_channels, \n",
    "                               args.GNN_num_layers, args.dropout).to(device)\n",
    "    if args.GNN_type == 'sage':\n",
    "        model = SAGE(data_train.x.shape[1], args.GNN_hidden_channels, args.GNN_out_channels, \n",
    "                           args.GNN_num_layers, args.dropout).to(device)\n",
    "    if args.GNN_type == 'autoencoder':\n",
    "        model = GAE(GAutoEncoder(data_train.x.shape[1], args.GNN_hidden_channels, args.GNN_out_channels,\n",
    "                                   args.GNN_num_layers, args.dropout).to(device))\n",
    "        \n",
    "    if args.subgraph_feature_type == 'cnn2':\n",
    "        model_predictor = predictor_model_cnn(num_subgraph_nodes, args.GNN_out_channels, args.linear_hidden_channels, \n",
    "                                          args.linear_num_layers, args.dropout).to(device)\n",
    "    else:\n",
    "        model_predictor = predictor_model(args.GNN_out_channels, args.linear_hidden_channels, \n",
    "                                          args.linear_num_layers, args.dropout).to(device)\n",
    "    \n",
    "    if args.subgraph_feature_type == 'cnn':\n",
    "        model_subgraph = subg2vec_model(num_subgraph_nodes, args.GNN_out_channels, args.subg2vec_hidden_channels, \n",
    "                                          args.subg2vec_out_channels, args.dropout).to(device)\n",
    "    \n",
    "    coeff = args.coefficient\n",
    "\n",
    "#*******train and test model*******    \n",
    "    if args.fold==True:\n",
    "        for run in range(args.no_start_run , 1 + args.runs):\n",
    "        \n",
    "            # Define the K-fold Cross Validator\n",
    "\n",
    "            kfold = KFold(n_splits=args.kfolds, shuffle=False)\n",
    "\n",
    "            train_ids=[]\n",
    "            test_ids=[]\n",
    "\n",
    "            for fold, (train_id, test_id) in enumerate(kfold.split(edge_pos)):\n",
    "                train_ids.append(train_id)\n",
    "                test_ids.append(test_id)\n",
    "\n",
    "            # K-fold Cross Validation model evaluation\n",
    "            results_folds=[]\n",
    "            for fold in range(args.kfolds):\n",
    "                print('--------------------------------------------------------------')\n",
    "                print('--------------------------------------------------------------')\n",
    "                print(f'FOLD {fold}')\n",
    "                print('--------------------------------')\n",
    "\n",
    "                split_edge = {'train': {'edge':edge_pos[train_ids[fold]], \n",
    "                                'edge_neg':edge_neg[train_ids[fold]]},\n",
    "                                'test':{'edge':edge_pos[test_ids[fold]], \n",
    "                                'edge_neg':edge_neg[test_ids[fold]]} }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                model.reset_parameters()\n",
    "                model_predictor.reset_parameters()\n",
    "                if args.subgraph_feature_type == 'cnn':\n",
    "                    model_subgraph.reset_parameters()\n",
    "\n",
    "                optimizer = torch.optim.Adam(list(model.parameters()), lr=args.lr)\n",
    "                optimizer_predictor = torch.optim.Adam(list(model_predictor.parameters()), lr=args.lr)\n",
    "                if args.subgraph_feature_type == 'cnn':\n",
    "                    optimizer_subgraph = torch.optim.Adam(list(model_subgraph.parameters()), lr=args.lr)\n",
    "\n",
    "\n",
    "                max_accu = 0\n",
    "                min_loss = 1000\n",
    "                losses = []\n",
    "                for epoch in range(1, 1 + args.epochs):\n",
    "                    print(f'Epoch: {epoch:02d} and Fold: {fold:02d} and Run: {run:02d}' )\n",
    "#*******train*******\n",
    "                    print(datetime.now())\n",
    "                    print(data)\n",
    "                    if args.subgraph_feature_type == 'cnn':\n",
    "                        loss = train(model, model_predictor, data, A, split_edge, optimizer, optimizer_predictor, \n",
    "                                        num_subgraph_nodes, model_subgraph, optimizer_subgraph)\n",
    "                    else:\n",
    "                        loss = train(model, model_predictor, data, A, split_edge, optimizer, optimizer_predictor, \n",
    "                                        num_subgraph_nodes)\n",
    "\n",
    "                    print(datetime.now())\n",
    "\n",
    "\n",
    "                    min_loss = loss\n",
    "\n",
    "                    if epoch % coeff == 0 :\n",
    "\n",
    "                        torch.save(model.state_dict(), 'models/%s_%s_%s_model_%s_%s_%s_epochs%d_fold%d_run%d.pth' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, epoch, fold, run))\n",
    "\n",
    "                        torch.save(model_predictor.state_dict(), 'models/%s_%s_%s_model_predictor_%s_%s_%s_epochs%d_fold%d_run%d.pth' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, epoch, fold, run))\n",
    "\n",
    "                        if args.subgraph_feature_type == 'cnn':\n",
    "                            torch.save(model_subgraph.state_dict(), 'models/%s_%s_%s_model_subgraph_%s_%s_%s_epochs%d_fold%d_run%d.pth' \n",
    "                                        %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                            args.subgraph_feature_type, args.dist_type, epoch, fold, run))\n",
    "\n",
    "                        losses.append(min_loss)\n",
    "\n",
    "                    if epoch % args.eval_steps == 0:\n",
    "\n",
    "                        results = []\n",
    "                        for e in range(1,args.epochs+1):\n",
    "                            if e%coeff == 0:\n",
    "#*******test*******                               \n",
    "                                model.load_state_dict(torch.load('models/%s_%s_%s_model_%s_%s_%s_epochs%d_fold%d_run%d.pth' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, e, fold, run)))\n",
    "\n",
    "                                model_predictor.load_state_dict(torch.load('models/%s_%s_%s_model_predictor_%s_%s_%s_epochs%d_fold%d_run%d.pth' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, e, fold, run)))\n",
    "\n",
    "                                if args.subgraph_feature_type == 'cnn':\n",
    "                                    model_subgraph.load_state_dict(torch.load('models/%s_%s_%s_model_subgraph_%s_%s_%s_epochs%d_fold%d_run%d.pth' \n",
    "                                            %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                                args.subgraph_feature_type, args.dist_type, e, fold, run)))\n",
    "\n",
    "                                if args.subgraph_feature_type == 'cnn':\n",
    "                                    result = test(model, model_predictor, data, A, split_edge, num_subgraph_nodes, model_subgraph)\n",
    "                                else:\n",
    "                                    result = test(model, model_predictor, data, A, split_edge, num_subgraph_nodes)\n",
    "\n",
    "                                results.append(result)\n",
    "\n",
    "                                result_train = result['train']['accu_train']\n",
    "                                result_valid = 0\n",
    "                                result_test = result['test']['accu_test']\n",
    "                                \n",
    "                                \n",
    "#*******print results*******                                \n",
    "                                i = (int(e/coeff))-1\n",
    "                                with open('results/%s_%s_%s_model_%s_%s_%s_epochs%d_fold%d_run%d.csv' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, e, fold, run), mode='w') as csv_file:\n",
    "                                    fieldnames = ['loss', 'accu_train', 'accu_valid', 'accu_test']\n",
    "                                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                                    writer.writeheader()\n",
    "                                    writer.writerow({'loss':losses[i], 'accu_train': result_train, 'accu_valid': result_valid, \n",
    "                                                        'accu_test': result_test})\n",
    "\n",
    "\n",
    "                                print(f'Best Result in epochs {e:02d} fold {fold:02d} and Run {run:02d} ')\n",
    "                                print('---')\n",
    "\n",
    "\n",
    "                                print(f'Loss_model: {losses[i]:.4f}, '\n",
    "                                                f'Accu_Train_model: {result_train:.4f}, '\n",
    "                                                f'Accu_Valid_model: {result_valid:.4f}, '\n",
    "                                                f'Accu_Test_model: {result_test:.4f}')\n",
    "                                print('---')\n",
    "\n",
    "            result_fold = []  \n",
    "\n",
    "            for e in range(1,args.epochs+1):\n",
    "                if e%coeff == 0:\n",
    "                    sum_folds = np.zeros(4)\n",
    "                    for f in range(args.kfolds):   \n",
    "                        df_model = pd.read_csv('results/%s_%s_%s_model_%s_%s_%s_epochs%d_fold%d_run%d.csv' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, e, f, run))\n",
    "                        sum_folds = (sum_folds + df_model.values[0]).tolist()\n",
    "                    result_fold.append (sum_folds)\n",
    "\n",
    "\n",
    "\n",
    "            results_folds = (torch.tensor(result_fold))/args.kfolds\n",
    "\n",
    "\n",
    "            print('--------------------------------------------------------------------')\n",
    "            print(f'folds Results in Run {run:02d}')\n",
    "            print('---')\n",
    "\n",
    "            for e in range(1,args.epochs+1):\n",
    "                if e%coeff == 0:\n",
    "                    i = int(e/coeff)-1\n",
    "                    print(f'folds Results in Epoch: {e:02d}')\n",
    "                    print('---')\n",
    "                    print(f'Loss_model: {results_folds[i][0].item():.4f}, '\n",
    "                                f'Accu_Train_model: {results_folds[i][1].item():.4f}, '\n",
    "                                f'Accu_Valid_model: {results_folds[i][2].item():.4f}, '\n",
    "                                f'Accu_Test_model: {results_folds[i][3].item():.4f}')\n",
    "                    print('---')\n",
    "\n",
    "\n",
    "            max_results_model.append(results_folds)\n",
    "            print(max_results_model)\n",
    "            \n",
    "        sum_results = torch.zeros(4)\n",
    "        for run in range(args.runs):\n",
    "            sum_results = sum_results + torch.tensor(max_results_model[run])\n",
    "\n",
    "\n",
    "        final_result_model = sum_results/args.runs\n",
    "\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Final Results')\n",
    "        print('---')\n",
    "\n",
    "        for e in range(1,args.epochs+1):\n",
    "            if e%coeff == 0:\n",
    "\n",
    "                i = int(e/coeff)-1\n",
    "\n",
    "                print(f'Final Results Epoch: {e:02d}')\n",
    "                print('---')\n",
    "                print(f'Loss_model: {final_result_model[i][0].item():.4f}, '\n",
    "                                f'Accu_Train_model: {final_result_model[i][1].item():.4f}, '\n",
    "                                f'Accu_Valid_model: {final_result_model[i][2].item():.4f}, '\n",
    "                                f'Accu_Test_model: {final_result_model[i][3].item():.4f}')\n",
    "                print('---')\n",
    "\n",
    "\n",
    "                with open('results/%s_%s_%s_model_%s_%s_%s_epochs%d_fold%d_run%d_final.csv' \n",
    "                            %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                            args.subgraph_feature_type, args.dist_type, args.epochs, args.kfolds, args.runs) , mode='w') as csv_file:\n",
    "                    fieldnames = ['loss', 'accu_train', 'accu_valid', 'accu_test']\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    writer.writeheader()\n",
    "                    writer.writerow({'loss':final_result_model[i][0].item(), 'accu_train': final_result_model[i][1].item(), \n",
    "                                        'accu_valid': final_result_model[i][2].item(), 'accu_test': final_result_model[i][3].item()})\n",
    "        print(datetime.now())\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        max_results_model=[]\n",
    "        max_precision_recall_results=[]\n",
    "        for run in range(args.no_start_run , 1 + args.runs):\n",
    "            model.reset_parameters()\n",
    "            model_predictor.reset_parameters()\n",
    "            if args.subgraph_feature_type == 'cnn':\n",
    "                model_subgraph.reset_parameters()\n",
    "\n",
    "            optimizer = torch.optim.Adam(list(model.parameters()), lr=args.lr)\n",
    "            optimizer_predictor = torch.optim.Adam(list(model_predictor.parameters()), lr=args.lr)\n",
    "            if args.subgraph_feature_type == 'cnn':\n",
    "                optimizer_subgraph = torch.optim.Adam(list(model_subgraph.parameters()), lr=args.lr)\n",
    "\n",
    "\n",
    "            max_accu = 0\n",
    "            min_loss = 1000\n",
    "            losses = []\n",
    "            for epoch in range(1, 1 + args.epochs):\n",
    "                print(f'Epoch: {epoch:02d} and Run: {run:02d}' )\n",
    "#*******train*******\n",
    "                print(datetime.now())\n",
    "\n",
    "                if args.subgraph_feature_type == 'cnn':\n",
    "                    loss = train(model, model_predictor, data_train, A_train, split_edge, optimizer, optimizer_predictor, \n",
    "                                    num_subgraph_nodes, model_subgraph, optimizer_subgraph)\n",
    "                else:\n",
    "                    loss = train(model, model_predictor, data_train, A_train, split_edge, optimizer, optimizer_predictor, \n",
    "                                    num_subgraph_nodes)\n",
    "\n",
    "                print(datetime.now())\n",
    "\n",
    "                min_loss = loss\n",
    "\n",
    "                if epoch % coeff == 0 :\n",
    "\n",
    "                    torch.save(model.state_dict(), 'models/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                    args.subgraph_feature_type, args.dist_type, epoch, run))\n",
    "\n",
    "                    torch.save(model_predictor.state_dict(), 'models/%s_%s_%s_model_predictor_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                    args.subgraph_feature_type, args.dist_type, epoch, run))\n",
    "\n",
    "                    if args.subgraph_feature_type == 'cnn':\n",
    "\n",
    "                        torch.save(model_subgraph.state_dict(), 'models/%s_%s_%s_model_subgraph_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                    %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                        args.subgraph_feature_type, args.dist_type, epoch, run))\n",
    "\n",
    "                    losses.append(min_loss)\n",
    "\n",
    "                if epoch % args.eval_steps == 0:\n",
    "\n",
    "                    results = []\n",
    "                    precision_recall_results = []\n",
    "                    \n",
    "                    \n",
    "                    for e in range(1,args.epochs+1):\n",
    "                        if e%coeff == 0:\n",
    "#*******test*******                               \n",
    "                            model.load_state_dict(torch.load('models/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                    args.subgraph_feature_type, args.dist_type, e, run)))\n",
    "\n",
    "                            model_predictor.load_state_dict(torch.load('models/%s_%s_%s_model_predictor_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                    args.subgraph_feature_type, args.dist_type, e, run)))\n",
    "\n",
    "                            if args.subgraph_feature_type == 'cnn':\n",
    "                                model_subgraph.load_state_dict(torch.load('models/%s_%s_%s_model_subgraph_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                        %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                            args.subgraph_feature_type, args.dist_type, e, run)))\n",
    "\n",
    "                            if args.subgraph_feature_type == 'cnn':\n",
    "                                result = test(model, model_predictor, data_train, data_test,A_train, A_test, split_edge, num_subgraph_nodes, model_subgraph)\n",
    "                            else:\n",
    "                                result = test(model, model_predictor, data_train, data_test,A_train, A_test, split_edge, num_subgraph_nodes)\n",
    "\n",
    "                            \n",
    "\n",
    "                            \n",
    "                            \n",
    "                            accu_train = result['train']['accu_train']\n",
    "                            accu_test = result['test']['accu_test']\n",
    "                            \n",
    "                            recall_train = result['train']['recall_train']\n",
    "                            recall_test = result['test']['recall_test']\n",
    "\n",
    "                            precision_train = result['train']['precision_train']\n",
    "                            precision_test = result['test']['precision_test']\n",
    "\n",
    "                            f1_train = result['train']['f1_train']\n",
    "                            f1_test = result['test']['f1_test']\n",
    "\n",
    "                            avg_precision_train = result['train']['avg_precision_train']\n",
    "                            avg_precision_test = result['test']['avg_precision_test']\n",
    "\n",
    "                            pr_auc_train = result['train']['pr_auc_train']\n",
    "                            pr_auc_test = result['test']['pr_auc_test']\n",
    "                            \n",
    "                            \n",
    "                            precision_list_train = result['train']['precision_list_train']\n",
    "                            recall_list_train = result['train']['recall_list_train']\n",
    "                            \n",
    "                            precision_list_test = result['test']['precision_list_test']\n",
    "                            recall_list_test = result['test']['recall_list_test']\n",
    "                            \n",
    "#*******print results*******                                \n",
    "                            i = (int(e/coeff))-1\n",
    "                            with open('results/%s_%s_%s_model_%s_%s_%s_epochs%d_d_run%d.csv' \n",
    "                                %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                    args.subgraph_feature_type, args.dist_type, e, run), mode='w') as csv_file:\n",
    "                                fieldnames = ['loss', 'accu_train', 'accu_test', 'recall_train',\n",
    "                                                'recall_test', 'precision_train', 'precision_test',\n",
    "                                                'f1_train', 'f1_test', 'avg_precision_train',\n",
    "                                                'avg_precision_test', 'pr_auc_train', 'pr_auc_test',\n",
    "                                                'precision_list_train', 'recall_list_train',\n",
    "                                                'precision_list_test', 'recall_list_test']\n",
    "                                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                                writer.writeheader()\n",
    "                                writer.writerow({'loss':losses[i], 'accu_train': accu_train,  \n",
    "                                                    'accu_test': accu_test, 'recall_train':recall_train, \n",
    "                                                    'recall_test':recall_test, 'precision_train':precision_train,\n",
    "                                                    'precision_test':precision_test,\n",
    "                                                    'f1_train':f1_train,'f1_test':f1_test,\n",
    "                                                    'avg_precision_train':avg_precision_train,\n",
    "                                                    'avg_precision_test':avg_precision_test, 'pr_auc_train':pr_auc_train,\n",
    "                                                    'pr_auc_test':pr_auc_test,\n",
    "                                                    'precision_list_train':precision_list_train, \n",
    "                                                    'recall_list_train':recall_list_train,\n",
    "                                                    'precision_list_test':precision_list_test, \n",
    "                                                    'recall_list_test':recall_list_test})\n",
    "                            \n",
    "                            \n",
    "                            print(f'Best Result in epochs {e:02d} and Run {run:02d} ')\n",
    "                            print('---')\n",
    "\n",
    "\n",
    "                            print(f'Loss_model: {losses[i]:.4f}')\n",
    "                            \n",
    "                            print(f'Accu_Train: {accu_train:.4f},     '\n",
    "                                    f'Accu_Test: {accu_test:.4f}')\n",
    "                            \n",
    "                            print(f'Recall_Train: {recall_train:.4f},     '\n",
    "                                    f'Recall_Test: {recall_test:.4f}')\n",
    "                            \n",
    "                            print(f'Precision_Train: {precision_train:.4f},     '\n",
    "                                    f'Precision_Test: {precision_test:.4f}')\n",
    "                            \n",
    "                            print(f'F1_Score_Train: {f1_train:.4f},     ' \n",
    "                                    f'F1_Score_Test: {f1_test:.4f}')\n",
    "                            \n",
    "                            print(f'Avrage_precision_Train: {avg_precision_train:.4f},     ' \n",
    "                                    f'Avrage_precision_Test: {avg_precision_test:.4f}')\n",
    "                            \n",
    "                            print(f'Precision-Recall_AUC_Train: {pr_auc_train:.4f},     '\n",
    "                                    f'Precision-Recall_AUC_Test: {pr_auc_test:.4f}')\n",
    "                            \n",
    "                            print('---')\n",
    "                                \n",
    "                    \n",
    "                    \n",
    "        results=[]\n",
    "\n",
    "        for r in range(1,args.runs+1):\n",
    "            result=[]\n",
    "            for e in range(1,args.epochs+1):\n",
    "\n",
    "                if (e)%args.coefficient==0:\n",
    "                    df = pd.read_csv('results/%s_%s_%s_model_%s_%s_%s_epochs%d_d_run%d.csv' \n",
    "                            %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                args.subgraph_feature_type, args.dist_type, e, r))\n",
    "                    result.append([df['loss'].item(), df['accu_train'].item(), df['accu_test'].item(), df['recall_train'].item(),\n",
    "                                        df['recall_test'].item(), df['precision_train'].item(), df['precision_test'].item(),\n",
    "                                        df['f1_train'].item(), df['f1_test'].item(), df['avg_precision_train'].item(),\n",
    "                                        df['avg_precision_test'].item(), df['pr_auc_train'].item(), df['pr_auc_test'].item()])\n",
    "            results.append(result)\n",
    "        sum_results = torch.zeros([13])\n",
    "\n",
    "        for r in range(args.runs):\n",
    "            sum_results = sum_results+torch.tensor(results[r])\n",
    "        \n",
    "        final_result_model = sum_results/args.runs\n",
    "        \n",
    "        \n",
    "        print('--------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Final Results')\n",
    "        print('---')\n",
    "\n",
    "        for e in range(1,args.epochs+1):\n",
    "            if e%coeff == 0:\n",
    "\n",
    "                i = int(e/coeff)-1\n",
    "\n",
    "                print(f'Final Results Epoch: {e:02d}')\n",
    "                print('---')\n",
    "                print(f'Loss_model: {final_result_model[i][0].item():.4f} ')\n",
    "                \n",
    "                print(f'Accu_Train: {final_result_model[i][1].item():.4f},     '\n",
    "                        f'Accu_Test: {final_result_model[i][2].item():.4f}')\n",
    "                \n",
    "                print(f'Recall_Train: {final_result_model[i][3].item():.4f},     '\n",
    "                        f'Recall_Test: {final_result_model[i][4].item():.4f}')\n",
    "\n",
    "                print(f'Precision_Train: {final_result_model[i][5].item():.4f},     '\n",
    "                        f'Precision_Test: {final_result_model[i][6].item():.4f}')\n",
    "\n",
    "                print(f'F1_Score_Train: {final_result_model[i][7].item():.4f},     ' \n",
    "                        f'F1_Score_Test: {final_result_model[i][8].item():.4f}')\n",
    "\n",
    "                print(f'Avrage_precision_Train: {final_result_model[i][9].item():.4f},     ' \n",
    "                        f'Avrage_precision_Test: {final_result_model[i][10].item():.4f}')\n",
    "\n",
    "                print(f'Precision-Recall_AUC_Train: {final_result_model[i][11].item():.4f},     '\n",
    "                        f'Precision-Recall_AUC_Test: {final_result_model[i][12].item():.4f}')\n",
    "                print('---')\n",
    "\n",
    "\n",
    "                with open('results/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d_final.csv' \n",
    "                            %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                            args.subgraph_feature_type, args.dist_type, e, args.runs) , mode='w') as csv_file:\n",
    "                    fieldnames = ['loss', 'accu_train', 'accu_test', 'recall_train',\n",
    "                                    'recall_test', 'precision_train', 'precision_test',\n",
    "                                    'f1_train', 'f1_test', 'avg_precision_train',\n",
    "                                    'avg_precision_test', 'pr_auc_train', 'pr_auc_test']\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    writer.writeheader()\n",
    "                    writer.writerow({'loss':final_result_model[i][0].item(), \n",
    "                                        'accu_train': final_result_model[i][1].item(), \n",
    "                                        'accu_test': final_result_model[i][2].item(),  \n",
    "                                        'recall_train':final_result_model[i][3].item(), \n",
    "                                        'recall_test':final_result_model[i][4].item(), \n",
    "                                        'precision_train':final_result_model[i][5].item(),\n",
    "                                        'precision_test':final_result_model[i][6].item(),\n",
    "                                        'f1_train':final_result_model[i][7].item(),\n",
    "                                        'f1_test':final_result_model[i][8].item(),\n",
    "                                        'avg_precision_train':final_result_model[i][9].item(),\n",
    "                                        'avg_precision_test':final_result_model[i][10].item(), \n",
    "                                        'pr_auc_train':final_result_model[i][11].item(),\n",
    "                                        'pr_auc_test':final_result_model[i][12].item(),\n",
    "                                        })\n",
    "        print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(dataset_name='karate', dist_type='cos', subgraph_type='hhop',\n",
    "                           feature_type='node2vec', subgraph_feature_type='cnn', GNN_type='gcn', epochs=200,\n",
    "                           eval_steps=200, no_start_run=1, runs=10, coefficient=50, fold=False, lr=0.01)\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
